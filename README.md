# ğŸš— PrÃ¡ctica de IntegraciÃ³n: Pipeline DataOps con Apache Airflow

Este proyecto implementa una plataforma **ETL (Extract, Transform, Load)** orquestada profesionalmente para integrar datos de trÃ¡fico y meteorologÃ­a del PaÃ­s Vasco.

El objetivo es generar un dataset analÃ­tico que permita estudiar la correlaciÃ³n entre accidentes y clima, utilizando una arquitectura moderna basada en **microservicios (Docker)** y **Apache Airflow**.

---

## ğŸ“‹ CaracterÃ­sticas del Proyecto

* **Arquitectura:** DataOps basada en Contenedores.
* **OrquestaciÃ³n:** **Apache Airflow 2.10** gestionando el ciclo de vida del dato.
* **Fuentes de Datos:**
    * **API TrÃ¡fico (OpenData Euskadi):** Incidencias reales.
    * **API Euskalmet:** Predicciones meteorolÃ³gicas.
    * **Mapeo Local:** CSV para normalizaciÃ³n de zonas.
* **Capacidades Avanzadas:**
    * **Backfilling:** Carga automÃ¡tica de datos histÃ³ricos (mes completo) al activar el pipeline.
    * **OptimizaciÃ³n:** Consultas meteorolÃ³gicas inteligentes (solo zonas afectadas).
    * **Calidad del Dato:** ValidaciÃ³n de esquemas con **Pandera**.
    * **Resiliencia:** Reintentos automÃ¡ticos ante fallos de red.

---

## ğŸ“‚ Estructura del Proyecto

La estructura sigue el estÃ¡ndar de Airflow en Docker:

```text
practica_1/
â”œâ”€â”€ dags/                  # LÃ³gica del Pipeline (CÃ³digo Python)
â”‚   â””â”€â”€ etl_trafico.py     # DAG que define el flujo ETL
â”œâ”€â”€ data/                  # Insumos estÃ¡ticos
â”‚   â””â”€â”€ mapping_municipios.csv  # Maestro de datos para cruces
â”œâ”€â”€ output/                # RESULTADOS (AquÃ­ aparecen los Excel generados)
â”œâ”€â”€ Apikey/                # Secretos
â”‚   â””â”€â”€ privateKey.pem     # (NECESARIO) Clave privada RSA
â”œâ”€â”€ Dockerfile             # Imagen personalizada de Airflow
â”œâ”€â”€ docker-compose.yaml    # DefiniciÃ³n de la infraestructura (Webserver, Scheduler, DB)
â”œâ”€â”€ logs/                  # Logs de ejecuciÃ³n (generado automÃ¡ticamente)
â”œâ”€â”€ plugins/               # Plugins de Airflow
â””â”€â”€ .env                   # Variables de entorno
```

---

## ğŸš€ Requisitos Previos* **Docker** y **Docker Compose** instalados.
* Clave privada (`privateKey.pem`) colocada en la carpeta `Apikey/`. Puedes generar una desde [opendata.euskadi.eus](https://opendata.euskadi.eus/euskalmet-api/-/api-de-euskalmet/).

---

## ğŸ› ï¸ Instrucciones de Despliegue (DataOps)Sigue estos pasos estrictos para levantar la plataforma sin errores de permisos.

### 1. PreparaciÃ³n del EntornoDesde la terminal en la carpeta raÃ­z del proyecto (`practica_1`), asegÃºrate de que las carpetas de trabajo tengan los permisos correctos para que Docker pueda escribir en ellas (especialmente en Linux/Mac/WSL):

```bash
# Crear carpetas si no existen
mkdir -p logs plugins output data

# Asignar permisos de escritura (CRÃTICO para evitar errores de PermissionDenied)
sudo chmod -R 777 dags logs plugins output data Apikey

```

### 2. ConstrucciÃ³n y ArranqueLevanta la infraestructura completa (Scheduler, Webserver, Postgres e Inicializador):

```bash
sudo docker compose up --build

```

*Espera unos instantes hasta que veas en los logs que el `airflow-webserver` estÃ¡ escuchando.*

---

## ğŸ–¥ï¸ Uso de la Plataforma
ğŸ‘‰ **http://localhost:8080**

* **Usuario:** `admin`
* **ContraseÃ±a:** `admin`

Los archivos generados aparecerÃ¡n en tu carpeta local `output/` con el formato:
ğŸ“„ `reporte_diario_YYYYMMDD.xlsx`

---

## âœ… Calidad y OptimizaciÃ³nEl pipeline incluye controles estrictos:

1. **ValidaciÃ³n de Esquema (Pandera):** Cada ejecuciÃ³n verifica que:
* No falten municipios (`cityTown`).
* El tipo sea estrictamente "Accidente".
* La temperatura estÃ© en rangos fÃ­sicos lÃ³gicos (-15ÂºC a 50ÂºC).


2. **OptimizaciÃ³n de API:**
El script analiza los accidentes del dÃ­a y **solo descarga el clima de los municipios afectados**, reduciendo las peticiones a la API de Euskalmet de ~300 a ~10 por dÃ­a.

---

## ğŸ§¹ LimpiezaPara detener la plataforma y limpiar los contenedores:

```bash
docker compose down
```